{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/solomontessema/Agentic-AI-with-Python/blob/main/notebooks/Building%20with%20LangChain/Building_a_Conversational_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BQRkf3HklNe"
      },
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <td><img src=\"https://ionnova.com/img/ionnova_logo_name_2.png\" width=\"120px\"></td>\n",
        "    <td><h1>Minimal Conversational Agent</h1></td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSQaUTz_k6qO"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain-openai langgraph python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8NZ2_4NqzCx3",
        "outputId": "c3980d88-a59d-4f9b-b926-de49bc54fd29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It seems that I couldn't find any inventory details for sugar. It's possible that it may not be listed under that name. Would you like me to check for any related products, such as \"granulated sugar\" or \"brown sugar\"?\n",
            "Enter your message: exit\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import langgraph\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import Tool\n",
        "from langchain.agents import create_agent\n",
        "from langchain.agents.middleware import SummarizationMiddleware\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "\n",
        "def check_inventory(product_name: str) -> str:\n",
        "    product_name = product_name.lower()\n",
        "    if \"laptop\" in product_name:\n",
        "        return \"The Pro-X Laptop has 15 units in stock.\"\n",
        "    elif \"monitor\" in product_name:\n",
        "        return \"The Ultra HD Monitor is currently out of stock.\"\n",
        "    else:\n",
        "        return f\"Could not find inventory details for {product_name}.\"\n",
        "\n",
        "check_inventory_tool = Tool(\n",
        "    name=\"check_inventory\",\n",
        "    func=check_inventory,\n",
        "    description = \"Returns the current stock count for a given product name.\"\n",
        ")\n",
        "\n",
        "summarization_middleware = SummarizationMiddleware(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    trigger=('tokens', 1000),\n",
        "    keep=('messages', 5),\n",
        ")\n",
        "\n",
        "conversational_agent = create_agent(\n",
        "    model=llm,\n",
        "    tools=[check_inventory_tool],\n",
        "    system_prompt=\"You are a helpful and detailed customer service agent. You must use your tools whenever necessary to answer questions about inventory.\",\n",
        "    middleware=[summarization_middleware],\n",
        "    checkpointer=InMemorySaver(),\n",
        ")\n",
        "\n",
        "\n",
        "def run_chat(user_input: str):\n",
        "    config = {\"configurable\": {\"thread_id\": \"1001\"}}\n",
        "    response = conversational_agent.invoke(\n",
        "        {\"messages\": [HumanMessage(content=user_input)]},\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    ai_response = response[\"messages\"][-1].content\n",
        "    return ai_response\n",
        "\n",
        "\n",
        "while True:\n",
        "    user_question = input(\"Enter your message: \")\n",
        "    if user_question.lower() == \"exit\":\n",
        "        break\n",
        "    response = run_chat(user_question)\n",
        "    print(response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
